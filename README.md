# LangGraph-Ollama-Gradio-Agent

# üöÄ LangGraph Ollama Gradio Agent

This project shows how to build and run an **AI Agent** using:
- **LangGraph** for agent workflows
- **Ollama** for local LLMs like Mistral, Llama2, or Gemma
- **Gradio** for a simple web user interface

---

## üìÅ Folder Structure

langgraph_agent/
‚îú‚îÄ‚îÄ agent_graph.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore

yaml
Copy
Edit

---

## ‚úÖ How to Use ‚Äî **Full Process**

Follow these simple steps to set up, run, and deploy your agent.

---

## 1Ô∏è‚É£ **Clone this project**

First, clone your GitHub repo:


git clone https://github.com/<YOUR_USERNAME>/<YOUR_REPO>.git
cd <YOUR_REPO>
2Ô∏è‚É£ Create a virtual environment
Create and activate a Python virtual environment to keep your packages organized.

bash
Copy
Edit
# Create
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (macOS/Linux)
source venv/bin/activate
3Ô∏è‚É£ Install dependencies
Install all required Python packages:

bash
Copy
Edit
pip install -r requirements.txt
4Ô∏è‚É£ Pull and run Ollama models
Make sure Ollama is installed and running.
Start by pulling a model like mistral:

bash
Copy
Edit
ollama pull mistral
ollama serve
ollama serve starts the Ollama server on localhost:11434 by default.

5Ô∏è‚É£ Run the Gradio Agent
Now launch your agent with:

bash
Copy
Edit
python agent_graph.py
Gradio will start and show a local link like:
Running on http://127.0.0.1:7860
Open that link in your browser to test your agent!

6Ô∏è‚É£ Push to GitHub
When everything works, push your project:

bash
Copy
Edit
git init
git add .
git commit -m "Initial commit: LangGraph Ollama Gradio Agent"
git branch -M main
git remote add origin https://github.com/<YOUR_USERNAME>/<YOUR_REPO>.git
git push -u origin main
‚úÖ ‚úÖ Optional: How It Works
agent_graph.py sets up:

OllamaLLM for your local model

LangGraph nodes to run prompts

Gradio as the user interface

When you type a question in Gradio:

It goes through the LangGraph

The graph calls Ollama for a reply

The answer shows in your browser!

